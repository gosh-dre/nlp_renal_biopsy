{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If JSON parsing errors in generated_answers.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if issues in processing\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from src.preprocessing.guidelines import EntityGuidelines\n",
    "from src.utils.general import write_metadata_file\n",
    "from src.utils.json import load_json, save_json\n",
    "from src.renal_biopsy.preprocessor import RenalBiopsyProcessor\n",
    "from src.renal_biopsy.qa import RenalBiopsyOllamaQA\n",
    "\n",
    "results_dir = \"src/renal_biopsy/data/runs/20241220_185154\" \n",
    "model_path = \"phi3.5:3.8b-mini-instruct-q8\"\n",
    "n_prototype = 1\n",
    "n_shots = 2\n",
    "include_guidelines = True\n",
    "\n",
    "args = {'root_dir': 'src/renal_biopsy', 'model_name': model_path, 'n_shots': n_shots, 'n_prototype': n_prototype, 'include_guidelines': include_guidelines}\n",
    "\n",
    "eg = EntityGuidelines(f'{args['root_dir']}/data/guidelines.xlsx')\n",
    "processor = RenalBiopsyProcessor(guidelines=eg)\n",
    "input_json = processor.create_input_json(\n",
    "    data_path=f\"{args['root_dir']}/data/full_data.xlsx\",\n",
    "    save_path=f\"{args['root_dir']}/data/real_input.json\",\n",
    "    full=True\n",
    ")\n",
    "annotated_json = load_json(f'{args['root_dir']}/data/output_report_first100.json')\n",
    "\n",
    "model = RenalBiopsyOllamaQA(model_path=model_path, root_dir=\"src/renal_biopsy\")\n",
    "generated_answers_json = load_json(f\"{results_dir}/generated_answers.json\") # generated_answers_modified.json\"\n",
    "predicted_json = model.convert_generated_answers_to_json(generated_answers=generated_answers_json, input_json=input_json, n_prototype=args['n_prototype'])\n",
    "\n",
    "# save predicted JSON\n",
    "predicted_json_path = os.path.join(results_dir, \"predicted_test.json\")\n",
    "save_json(predicted_json, predicted_json_path)\n",
    "print(f\"Predicted JSON saved to {predicted_json_path}\")\n",
    "\n",
    "# Prepare metadata storage\n",
    "metadata = {\n",
    "    \"args\": args,\n",
    "    \"annotation_start_time\": None,\n",
    "    \"annotation_end_time\": None,\n",
    "    \"evaluation_start_time\": None,\n",
    "    \"evaluation_end_time\": None,\n",
    "    \"score_per_report\": None,\n",
    "    \"final_score\": None\n",
    "}\n",
    "\n",
    "# Evaluate model\n",
    "metadata[\"evaluation_start_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "all_scores, score_per_report, final_score = model.evaluate(annotated_json, predicted_json, n_prototypes=args['n_prototype'])\n",
    "metadata[\"evaluation_end_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Save evaluation results\n",
    "scores_path = os.path.join(results_dir, \"evaluation_scores_test.json\")\n",
    "save_json(all_scores, scores_path)\n",
    "print(f\"Evaluation results saved to {scores_path}\")\n",
    "\n",
    "\n",
    "# Save final scores to metadata\n",
    "metadata[\"score_per_report\"] = score_per_report\n",
    "metadata[\"final_score\"] = final_score\n",
    "# Save metadata to a text file\n",
    "metadata_path = os.path.join(results_dir, \"metadata_test.txt\")\n",
    "write_metadata_file(metadata_path, metadata)\n",
    "print(f\"Metadata saved to {metadata_path}\")\n",
    "\n",
    "# time: 33 mins\n",
    "# errors = 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
