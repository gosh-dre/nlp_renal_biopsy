{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you may need to change paths to your own model runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "true_root_dir = Path().resolve().parent\n",
    "sys.path.append(str(true_root_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which ollama models you have available\n",
    "\n",
    "import ollama\n",
    "from src.utils.general import wrap_text\n",
    "# print(wrap_text())\n",
    "for model_info in ollama.list()['models']:\n",
    "    print(wrap_text(str(model_info), 200))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking LAAJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from src.evaluate.tests.single_laaj_experiment  import LAAJExperiment\n",
    "\n",
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def use_bert_to_compare(entity1, entity2, threshold=0.8):\n",
    "    model = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
    "    embeddings = model.encode([entity1, entity2])\n",
    "    similarity = cosine_similarity(\n",
    "        embeddings[0].reshape(1, -1),\n",
    "        embeddings[1].reshape(1, -1),\n",
    "    )[0][0]\n",
    "    print(f\"{entity1}, {entity2}: {similarity}\")\n",
    "    return similarity >= threshold\n",
    "\n",
    "def use_llm_to_compare(entity1: str, entity2: str, model: str = 'gemma2:2b', provider: str = 'ollama') -> bool:\n",
    "    \"\"\"Compare two medical entities using specified LLM.\"\"\"\n",
    "\n",
    "    # adj\n",
    "    default_false_phrases = [\"none\", \"None\", \"null\", \"Null\", \"nan\", \"NaN\"]\n",
    "    # TODO: safe option would be to go to default value for entity if any of these seen\n",
    "    if entity1 in default_false_phrases:\n",
    "        entity1 = \"0\"\n",
    "    if entity2 in default_false_phrases:\n",
    "        entity2 = \"0\"\n",
    "    \n",
    "    entity1 = entity1.lower()\n",
    "    entity2 = entity2.lower()\n",
    "    \n",
    "    if entity1 == entity2:\n",
    "        return True\n",
    "    if (entity1 == \"0\" and entity2 != \"0\") or (entity2 == \"0\" and entity1 != \"0\"):\n",
    "        return False\n",
    "    \n",
    "    # full\n",
    "    query = f\"\"\"\n",
    "    You are a renal biopsy expert.\n",
    "    Are the phrases \"{entity1}\" and \"{entity2}\" describing equivalent or similar concepts? Only answer True or False.\n",
    "    Answer based on the nouns, adjectives, or numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    # just adj\n",
    "    #query = f\"\"\"\n",
    "    #Are the phrases \"{entity1}\" and \"{entity2}\" describing equivalent or similar concepts? Only answer True or False.\n",
    "    #Answer based on the nouns, adjectives, or numbers.\n",
    "    #\"\"\"\n",
    "\n",
    "    # just expert\n",
    "    #query = f\"\"\"\n",
    "    #You are a renal biopsy expert.\n",
    "    #Are the phrases \"{entity1}\" and \"{entity2}\" describing equivalent or similar concepts? Only answer True or False.\n",
    "    #\"\"\"\n",
    "\n",
    "    # simple\n",
    "    #query = f\"\"\"\n",
    "    #Are the phrases \"{entity1}\" and \"{entity2}\" describing equivalent or similar concepts? Only answer True or False.\n",
    "    #\"\"\"\n",
    "\n",
    "    #query = f\"\"\"\n",
    "    #Are the phrases \"{entity1}\" and \"{entity2}\" describing equivalent or similar concepts? Only answer True or False.\n",
    "    #\"\"\"\n",
    "    \n",
    "    if provider == 'ollama':\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=query,\n",
    "            options={'temperature': 0, 'num_predict': 2, 'num_ctx': 1024}\n",
    "        )\n",
    "        return \"True\" in response['response']\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "def run_experiment(model_name='qwen2.5:1.5b-instruct-fp16', size=\"small\", save_files=False):\n",
    "    experiment = LAAJExperiment(partial(use_llm_to_compare, model=model_name), size, n_trials=1)\n",
    "    \n",
    "    # bert struggles with abbreviations, and doesn't always know what opposites are. it is symmetric though\n",
    "    #experiment = LAAJExperiment(use_bert_to_compare, size, n_trials=1)\n",
    "    \n",
    "    results_df = experiment.run_trials()\n",
    "    metrics = experiment.analyse_results(results_df)\n",
    "    fig = experiment.plot_results(metrics, results_df)\n",
    "\n",
    "    # Save results\n",
    "    if save_files:\n",
    "        results_df.to_csv('llm_judge_results.csv')\n",
    "        with open('llm_judge_metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        fig.savefig('llm_judge_results.png')\n",
    "    \n",
    "    return results_df, metrics\n",
    "\n",
    "# smollm:360m-instruct-v0.2-fp16\n",
    "# llama3.2:3b-instruct-q8_0\n",
    "# qwen2.5:1.5b-instruct-fp16 # 18 mins\n",
    "# gemma2:2b-instruct-fp16 # 27 mins\n",
    "results_df, metrics = run_experiment('llama3.2:3b-instruct-fp16', \"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['category'] == 'exact']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redoing evaluation for LLM methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.json import load_json\n",
    "from src.preprocessing.guidelines import EntityGuidelines\n",
    "from src.evaluate.alt_models import evaluate, calculate_entity_accuracy\n",
    "\n",
    "eg = EntityGuidelines(f'src/renal_biopsy/data/guidelines.xlsx')\n",
    "qa_gt_json = load_json(f\"src/renal_biopsy/data/output_report_first100.json\")\n",
    "\n",
    "gemma_100_json = load_json(f\"src/renal_biopsy/data/runs/main/gemma 2 True 1 20241209_190736/predicted.json\")\n",
    "llama_q8_100_json = load_json(f\"src/renal_biopsy/data/runs/main/llama q8 2 True 6 20241210_013000/predicted.json\")\n",
    "phi_q4_100_json = load_json(f\"src/renal_biopsy/data/runs/main/phi q4 2 True 9 20241213_170353/predicted.json\")\n",
    "phi_q8_100_json = load_json(f\"src/renal_biopsy/data/runs/main/phi q8 2 True 14 20241213_233231/predicted.json\")\n",
    "qwen_100_json = load_json(f\"src/renal_biopsy/data/runs/main/qwen 2 True 5 20241210_004135/predicted.json\") \n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, gemma_100_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)\n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, llama_q8_100_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)\n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, phi_q4_100_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)\n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, phi_q8_100_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)\n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, qwen_100_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from src.utils.general import write_metadata_file\n",
    "from src.utils.json import load_json, save_json\n",
    "from src.renal_biopsy.qa import RenalBiopsyOllamaQA\n",
    "from src.evaluate.alt_models import calculate_entity_accuracy\n",
    "from src.preprocessing.guidelines import EntityGuidelines\n",
    "\n",
    "def get_all_folders_from_root(root_path=\"../src/renal_biopsy/data/runs/main\"):\n",
    "    try:\n",
    "        folders = [entry.name for entry in os.scandir(root_path) if entry.is_dir()]\n",
    "        # print(folders)\n",
    "        return folders\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {root_path}\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied accessing: {root_path}\")\n",
    "\n",
    "def create_args(results_dir):\n",
    "    with open(f'{results_dir}/metadata_v4.txt', 'r') as file:\n",
    "        line = file.readline()  # Read the first line\n",
    "        \n",
    "    dict_str = line.replace('args: ', '')\n",
    "    args_dict = ast.literal_eval(dict_str)\n",
    "    return args_dict\n",
    "\n",
    "def rerun_via_args(args, results_dir):\n",
    "    annotated_json = load_json(f'../{args['root_dir']}/data/output_report_first100.json')\n",
    "\n",
    "    model = RenalBiopsyOllamaQA(model_path=args['model_name'], root_dir=\"../src/renal_biopsy\")\n",
    "    predicted_json = load_json(f\"{results_dir}/predicted.json\")\n",
    "\n",
    "    # Prepare metadata storage\n",
    "    metadata = {\n",
    "        \"args\": args,\n",
    "        \"annotation_start_time\": None,\n",
    "        \"annotation_end_time\": None,\n",
    "        \"evaluation_start_time\": None,\n",
    "        \"evaluation_end_time\": None,\n",
    "        \"score_per_report\": None,\n",
    "        \"final_score\": None\n",
    "    }\n",
    "\n",
    "    # Evaluate model\n",
    "    eg = EntityGuidelines(f'../src/renal_biopsy/data/guidelines.xlsx')\n",
    "    all_scores, score_per_report, final_score = model.evaluate(annotated_json, predicted_json, n_prototypes=args['n_prototype'])\n",
    "    entity_scores = calculate_entity_accuracy(all_scores, eg)\n",
    "\n",
    "    # Save evaluation results\n",
    "    scores_path = os.path.join(results_dir, \"evaluation_scores_redo.json\")\n",
    "    save_json(all_scores, scores_path)\n",
    "\n",
    "    # Save metadata to a text file\n",
    "    metadata[\"score_per_report\"] = score_per_report\n",
    "    metadata[\"final_score\"] = final_score\n",
    "    metadata_path = os.path.join(results_dir, \"metadata_redo.txt\")\n",
    "    write_metadata_file(metadata_path, metadata)\n",
    "    print(\"--- Finished ---\")\n",
    "\n",
    "def rerun_with_new_laaj_function(root_path):\n",
    "    folder_paths = get_all_folders_from_root(root_path)\n",
    "    for f in folder_paths:\n",
    "        results_dir = f\"{root_path}/{f}\"\n",
    "        args = create_args(results_dir)\n",
    "        print(args)\n",
    "        rerun_via_args(args, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reevaluate one model run\n",
    "results_dir = \"../src/renal_biopsy/data/runs/main/llama q8 2 True 6 20241210_013000\"\n",
    "args = create_args(results_dir)\n",
    "print(args)\n",
    "rerun_via_args(args, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reevaluate all model runs in a given folder\n",
    "root_path = \"../src/renal_biopsy/data/runs/main\"\n",
    "rerun_with_new_laaj_function(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redoing evaluation for alternative methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.json import load_json, convert_to_strings\n",
    "from src.preprocessing.guidelines import EntityGuidelines\n",
    "from src.evaluate.alt_models import evaluate, calculate_entity_accuracy\n",
    "\n",
    "eg = EntityGuidelines(f'src/renal_biopsy/data/guidelines.xlsx')\n",
    "\n",
    "ner_gt_json = load_json(f\"src/ner/annotations_output/annotations_first20.json\")\n",
    "qa_gt_json = load_json(f\"src/renal_biopsy/data/output_report_first100.json\")\n",
    "\n",
    "spacy20_json = convert_to_strings(load_json(f\"src/renal_biopsy/data/runs/alt/spacy_first20.json\"))\n",
    "spacy100_json = convert_to_strings(load_json(f\"src/renal_biopsy/data/runs/alt/spacy_first100.json\"))\n",
    "\n",
    "biobert20_json = convert_to_strings(load_json(f\"src/renal_biopsy/data/runs/alt/biobert_squad_first20.json\"))\n",
    "biobert100_json = convert_to_strings(load_json(f\"src/renal_biopsy/data/runs/alt/biobert_squad_first100.json\"))\n",
    "roberta20_json = convert_to_strings(load_json(f\"src/renal_biopsy/data/runs/alt/roberta_squad_first20.json\"))\n",
    "roberta100_json = convert_to_strings(load_json(f\"src/renal_biopsy/data/runs/alt/roberta_squad_first100.json\"))\n",
    "\n",
    "gliner_03_json = convert_to_strings(load_json(f\"src/renal_biopsy/data/runs/alt/gliner/gliner_first100_03_processed.json\"))\n",
    "\n",
    "nuextract_json = load_json(f\"src/renal_biopsy/data/runs/alt/nuextract_first20_transformed.json\")\n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, biobert100_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)\n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, roberta100_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)\n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, gliner_03_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)\n",
    "\n",
    "all_scores, scores_per_report, final_score = evaluate(qa_gt_json, spacy100_json, eg, 100)\n",
    "entity_scores = calculate_entity_accuracy(all_scores, eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma_100_json = load_json(f\"src/renal_biopsy/data/runs/main/gemma 2 True 1 20241209_190736/predicted.json\")\n",
    "# llama_q8_100_json = load_json(f\"src/renal_biopsy/data/runs/main/llama q8 2 True 6 20241210_013000/predicted.json\")\n",
    "# phi_q4_100_json = load_json(f\"src/renal_biopsy/data/runs/main/phi q4 2 True 9 20241213_170353/predicted.json\")\n",
    "# phi_q8_100_json = load_json(f\"src/renal_biopsy/data/runs/main/phi q8 2 True 14 20241213_233231/predicted.json\")\n",
    "# qwen_100_json = load_json(f\"src/renal_biopsy/data/runs/main/qwen 2 True 5 20241210_004135/predicted.json\") \n",
    "\n",
    "root_dir = f\"src/renal_biopsy/data/runs/main/gemma 2 True 1 20241209_190736\"\n",
    "answers = load_json(f\"{root_dir}/evaluation_scores_v6.json\")\n",
    "anno = load_json(f\"{root_dir}/annotated.json\")\n",
    "pred = load_json(f\"{root_dir}/predicted.json\")\n",
    "\n",
    "k = 'diagnosis'\n",
    "#k = 'chronic_change'\n",
    "\n",
    "list_to_use = []\n",
    "for a, p, a2 in zip(anno, pred, answers):\n",
    "    print(f\"anno: {a[k]}, pred: {p[k]}, answer: {a2[k]}\")\n",
    "    list_to_use.append((a[k], p[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "# from src.evaluate.laaj import use_llm_to_compare\n",
    "\n",
    "def use_llm_to_compare(entity1: str, entity2: str, model: str = 'gemma2:2b', provider: str = 'ollama') -> bool:\n",
    "    \"\"\"Compare two medical entities using specified LLM.\"\"\"\n",
    "\n",
    "    default_false_phrases = [\"none\", \"None\", \"null\", \"Null\", \"nan\", \"NaN\"]\n",
    "    # TODO: safe option would be to go to default value for entity if any of these seen\n",
    "    if entity1 in default_false_phrases:\n",
    "        entity1 = \"0\"\n",
    "    if entity2 in default_false_phrases:\n",
    "        entity2 = \"0\"\n",
    "    \n",
    "    entity1 = entity1.lower()\n",
    "    entity2 = entity2.lower()\n",
    "    \n",
    "    if entity1 == entity2:\n",
    "        return True\n",
    "    if (entity1 == \"0\" and entity2 != \"0\") or (entity2 == \"0\" and entity1 != \"0\"):\n",
    "        return False\n",
    "\n",
    "    query = f\"\"\"\n",
    "    You are a renal biopsy expert.\n",
    "    Are the phrases \"{entity1}\" and \"{entity2}\" describing equivalent or similar concepts? Only answer True or False.\n",
    "    Answer based on the nouns, adjectives, or numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    if provider == 'ollama':\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=query,\n",
    "            options={'temperature': 0, 'num_predict': 2, 'num_ctx': 1024}\n",
    "        )\n",
    "        # print(response['response'])\n",
    "        return \"True\" in response['response']\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "for e1, e2 in list_to_use:\n",
    "    # llama3.2:3b-instruct-fp16\n",
    "    # qwen2:7b-instruct-q6_K\n",
    "    truth_value = use_llm_to_compare(e1, e2, \"llama3.2:3b-instruct-fp16\")\n",
    "    print(f\"{e1} ||| {e2} -> {truth_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gosh-llm-2 (ipython)",
   "language": "python",
   "name": "gosh-llm-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
