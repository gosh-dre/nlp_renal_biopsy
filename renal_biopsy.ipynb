{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from src.utils.general import wrap_text\n",
    "# print(wrap_text())\n",
    "for model_info in ollama.list()['models']:\n",
    "    print(wrap_text(str(model_info), 200))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.guidelines import EntityGuidelines\n",
    "from src.renal_biopsy.preprocessor import RenalBiopsyProcessor\n",
    "\n",
    "root_data_dir = \"src/renal_biopsy/data\"\n",
    "guidelines = EntityGuidelines(f'{root_data_dir}/guidelines.xlsx')\n",
    "processor = RenalBiopsyProcessor(guidelines=guidelines)\n",
    "\n",
    "input_json = processor.create_input_json(\n",
    "    data_path=f\"{root_data_dir}/full_data.xlsx\",\n",
    "    save_path=f\"{root_data_dir}/input.json\",\n",
    "    full=True\n",
    ")\n",
    "\n",
    "segmented_reports = processor.process_all_reports_real(f\"{root_data_dir}/full_data.xlsx\")\n",
    "filtered_reports, microscopy_sections, conclusion_sections = processor.extract_valid_sections(\n",
    "    segmented_reports, \n",
    "    required_sections=['MICROSCOPY', 'CONCLUSION']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.eda import MedicalReportEDA\n",
    "\n",
    "eda = MedicalReportEDA()\n",
    "\n",
    "stats = eda.analyse_section_lengths(segmented_reports, exclude_keys=['entity_key'])\n",
    "\n",
    "stats = eda.calculate_report_statistics(\n",
    "    reports=segmented_reports,\n",
    "    section_keys=['MICROSCOPY', 'CONCLUSION']\n",
    ")\n",
    "# all sections\n",
    "# stats = eda.calculate_report_statistics(reports=segmented_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.analyse_word_distributions(microscopy_sections, f'Microscopy Section (n={len(microscopy_sections)})')\n",
    "eda.analyse_word_distributions(microscopy_sections, f'Microscopy Section (n={len(microscopy_sections)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of patients\n",
    "import pandas as pd\n",
    "sample_data = pd.read_excel(f\"{root_data_dir}/full_data.xlsx\")\n",
    "len(sample_data['project_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as lev\n",
    "\n",
    "highlight_words = ['glomeruli', 'medulla', 'cortex', 'fibrosis', 'sclerosed', 'chronic', 'interstitial', 'tubular', 'atrophy']\n",
    "word_freq_df = eda.analyse_word_frequencies_spacy(\n",
    "    segmented_reports, \n",
    "    'MICROSCOPY',\n",
    "    highlight_words=highlight_words,\n",
    "    n_terms=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for misspellings of main words\n",
    "misspellings = {}\n",
    "for correct_word in highlight_words:\n",
    "    misspellings[correct_word] = word_freq_df['Word'].apply(lambda x: lev.distance(x, correct_word) <= 3)\n",
    "\n",
    "# Print misspellings\n",
    "for correct_word, matches in misspellings.items():\n",
    "    print(f\"\\nPossible misspellings of '{correct_word}':\")\n",
    "    print(word_freq_df[matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['and', 'are', 'but', 'in', 'is', 'no', 'of', 'the', 'there', 'with', 'seen', 'show', 'shows', 'to', 'which']\n",
    "eda.analyse_tfidf(microscopy_sections[:40], n_terms=30, custom_stop_words=custom_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case of JSON parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of renal biopsy histopathology reports: 2462\n",
      "Number of reports after SPECIMEN keyword filtering: 2128\n",
      "Predicted JSON saved to src/renal_biopsy/data/runs/20241220_185154\\predicted2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating predictions: 100%|███████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to src/renal_biopsy/data/runs/20241220_185154\\evaluation_scores.json\n",
      "Metadata saved to src/renal_biopsy/data/runs/20241220_185154\\metadata.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if issues in processing\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from src.preprocessing.guidelines import EntityGuidelines\n",
    "from src.utils.general import write_metadata_file\n",
    "from src.utils.json import load_json, save_json\n",
    "from src.renal_biopsy.preprocessor import RenalBiopsyProcessor\n",
    "from src.renal_biopsy.qa import RenalBiopsyOllamaQA\n",
    "\n",
    "model_path = \"phi3.5:3.8b-mini-instruct-q8\"\n",
    "# src\\renal_biopsy\\data\\runs\\20241214_010916\n",
    "# src\\renal_biopsy\\data\\runs\\20241220_185154\n",
    "results_dir = \"src/renal_biopsy/data/runs/20241220_185154\" \n",
    "n_prototype = 1\n",
    "\n",
    "args = {'root_dir': 'src/renal_biopsy', 'model_name': 'llama3.2:3b-instruct-q8_0', 'n_shots': 2, 'n_prototype': n_prototype, 'include_guidelines': True}\n",
    "\n",
    "eg = EntityGuidelines(f'{args['root_dir']}/data/guidelines.xlsx')\n",
    "processor = RenalBiopsyProcessor(guidelines=eg)\n",
    "input_json = processor.create_input_json(\n",
    "    data_path=f\"{args['root_dir']}/data/full_data.xlsx\",\n",
    "    save_path=f\"{args['root_dir']}/data/real_input.json\",\n",
    "    full=True\n",
    ")\n",
    "annotated_json = load_json(f'{args['root_dir']}/data/output_report_first100.json')\n",
    "\n",
    "model = RenalBiopsyOllamaQA(model_path=model_path, root_dir=\"src/renal_biopsy\")\n",
    "generated_answers_json = load_json(f\"{results_dir}/generated_answers.json\") # generated_answers_modified.json\"\n",
    "predicted_json = model.convert_generated_answers_to_json(generated_answers=generated_answers_json, input_json=input_json, n_prototype=args['n_prototype'])\n",
    "\n",
    "# save predicted JSON\n",
    "predicted_json_path = os.path.join(results_dir, \"predicted2.json\")\n",
    "save_json(predicted_json, predicted_json_path)\n",
    "print(f\"Predicted JSON saved to {predicted_json_path}\")\n",
    "\n",
    "# Prepare metadata storage\n",
    "metadata = {\n",
    "    \"args\": args,\n",
    "    \"annotation_start_time\": None,\n",
    "    \"annotation_end_time\": None,\n",
    "    \"evaluation_start_time\": None,\n",
    "    \"evaluation_end_time\": None,\n",
    "    \"score_per_report\": None,\n",
    "    \"final_score\": None\n",
    "}\n",
    "\n",
    "# Evaluate model\n",
    "metadata[\"evaluation_start_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "all_scores, score_per_report, final_score = model.evaluate(annotated_json, predicted_json, n_prototypes=args['n_prototype'])\n",
    "metadata[\"evaluation_end_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Save evaluation results\n",
    "scores_path = os.path.join(results_dir, \"evaluation_scores.json\")\n",
    "save_json(all_scores, scores_path)\n",
    "print(f\"Evaluation results saved to {scores_path}\")\n",
    "\n",
    "\n",
    "# Save final scores to metadata\n",
    "metadata[\"score_per_report\"] = score_per_report\n",
    "metadata[\"final_score\"] = final_score\n",
    "# Save metadata to a text file\n",
    "metadata_path = os.path.join(results_dir, \"metadata.txt\")\n",
    "write_metadata_file(metadata_path, metadata)\n",
    "print(f\"Metadata saved to {metadata_path}\")\n",
    "\n",
    "# time: 33 mins\n",
    "# errors = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from src.automated_annotation.prompts import GUIDELINE_CREATION_TASK, GUIDELINE_EXAMPLE\n",
    "from src.automated_annotation.prompts import ENTITY_IDENTIFICATION_TASK\n",
    "from src.automated_annotation.prompts import SECTION_HEADER_IDENTIFIER_TASK\n",
    "from src.automated_annotation.prompts import create_few_shots_prompt\n",
    "\n",
    "report_strings_list = []\n",
    "for i, report in enumerate(input_json):\n",
    "    if i == 3:\n",
    "        break\n",
    "    report_strings_list.append(\n",
    "        f\"\"\"--- PATIENT {i}'S REPORT ---\n",
    "        \"MICROSCOPY SECTION: {report['microscopy_section']}\n",
    "        CONCLUSION SECTION: {report['conclusion_section']}\"\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "report_strings = \"\\n\".join(report_strings_list)\n",
    "\n",
    "\n",
    "guideline_creation_prompt = f\"{GUIDELINE_CREATION_TASK} \\n {report_strings}\"\n",
    "# entity_identification_prompt = f\"{ENTITY_IDENTIFICATION_TASK}\\n --- REPORTS ---\\n {report_strings}\"\n",
    "# section_identifier_prompt = f\"{SECTION_HEADER_IDENTIFIER_TASK}\\n --- REPORTS ---\\n {report_strings}\"\n",
    "few_shot_generation_prompt = # create_few_shots_prompt(3) # num_predict=1500\n",
    "\n",
    "# gemma2:2b-instruct-fp16\n",
    "# llama3.2:3b-instruct-q8_0\n",
    "answer = ollama.generate(\n",
    "    model=\"llama3.2:3b-instruct-q8_0\",\n",
    "    prompt=guideline_creation_prompt,\n",
    "    options={'temperature': 0, 'num_predict': 800}\n",
    ")\n",
    "\n",
    "from src.utils.general import wrap_text\n",
    "# print(wrap_text(guideline_creation_prompt, 100))\n",
    "print(wrap_text(answer['response'], 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### finding common words across all reports\n",
    "# - could probably do this with a prompt in some way (and produce fig along the way)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "def plot_word_frequencies(segmented_reports, n_most_common=40):\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Custom stop words plus spaCy defaults\n",
    "    stop_words = set(nlp.Defaults.stop_words).union({\n",
    "        # standard\n",
    "        'and', 'are', 'but', 'in', 'is', 'no', 'of', \n",
    "        'the', 'there', 'with', 'seen', 'show', 'shows', \n",
    "        'to', 'which',\n",
    "\n",
    "        # medical NOTE: issue here is they might be section headers\n",
    "        'biopsy', 'paediatric', 'clinical', 'pathology', 'consultant',\n",
    "        'dr', 'measuring', 'changes',\n",
    "    })\n",
    "\n",
    "    # Process all texts\n",
    "    all_words = []\n",
    "    for text in segmented_reports:\n",
    "        doc = nlp(text.lower())\n",
    "        words = [token.text for token in doc \n",
    "                if token.is_alpha and token.text not in stop_words]\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Count frequencies\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(word_freq.most_common(n_most_common), \n",
    "                     columns=['Word', 'Frequency'])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df, x='Frequency', y='Word', color='blue')\n",
    "    plt.title(f\"Top {n_most_common} Words (Total Reports: {len(segmented_reports)})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read and process data\n",
    "sample_data = pd.read_excel(\"src/renal_biopsy/data/full_data.xlsx\")\n",
    "segmented_reports = sample_data['content'].tolist()\n",
    "\n",
    "# Generate plot\n",
    "word_freq_df = plot_word_frequencies(segmented_reports, n_most_common=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### analyse post-disagreement analysis\n",
    "# - less relevant because if I make the comments, I can probably see the pattern. want llm to do itself without comments\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyse_json_by_entity(data):\n",
    "    # Create a dictionary to store comments by entity type\n",
    "    entity_comments = defaultdict(list)\n",
    "    \n",
    "    # Iterate through all reports\n",
    "    for report_id, report_content in data.items():\n",
    "        # Iterate through entities in each report\n",
    "        for entity_name, entity_data in report_content.items():\n",
    "            # Store the comment along with the report ID for context\n",
    "            if \"comment\" in entity_data:\n",
    "                entity_comments[entity_name].append({\n",
    "                    \"report_id\": report_id,\n",
    "                    \"comment\": entity_data[\"comment\"],\n",
    "                    \"pred1\": entity_data[\"pred1\"],\n",
    "                    \"pred2\": entity_data[\"pred2\"],\n",
    "                    \"match\": entity_data[\"match\"]\n",
    "                })\n",
    "    \n",
    "    return entity_comments\n",
    "\n",
    "def generate_analysis_prompt(entity_comments):\n",
    "    prompt = \"Based on the analysis of prediction mismatches, here are the issues by entity type:\\n\\n\"\n",
    "    \n",
    "    for entity, comments in entity_comments.items():\n",
    "        prompt += f\"\"\"Entity: {entity}\\n\n",
    "        Issues observed:\\n\"\"\"\n",
    "        for item in comments:\n",
    "            prompt += f\"- Report {item['report_id']}: {item['comment']}\\n\"\n",
    "            prompt += f\"  Pred1: {item['pred1']}, Pred2: {item['pred2']}, Match: {item['match']}\\n\"\n",
    "        prompt += f\"\"\"\\nPlease analyse these patterns and suggest:\\n\n",
    "        1. Common error patterns in the predictions\\n\n",
    "        2. Specific rules or validation checks that could be implemented\\n\n",
    "        3. Data quality or annotation guidelines that might need refinement\\n\\n\n",
    "        \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    \"report_0\": {\n",
    "        \"medulla_present\": {\n",
    "            \"pred1\": \"False\",\n",
    "            \"pred2\": \"True\",\n",
    "            \"match\": False,\n",
    "            \"comment\": \"Cortex is mentioned and medulla not mentioned. Therefore this should be false\"\n",
    "        }\n",
    "    },\n",
    "    \"report_1\": {\n",
    "        \"n_total\": {\n",
    "            \"pred1\": \"1\",\n",
    "            \"pred2\": \"null\",\n",
    "            \"match\": False,\n",
    "            \"comment\": \"Wrong null\"\n",
    "        },\n",
    "        \"n_segmental\": {\n",
    "            \"pred1\": \"0\",\n",
    "            \"pred2\": \"null\",\n",
    "            \"match\": False,\n",
    "            \"comment\": \"Should put 0\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "data = load_json(f\"src/renal_biopsy/data/runs/20241215_164609/comparison_comments_initial.json\")\n",
    "\n",
    "# Process the data\n",
    "entity_comments = analyse_json_by_entity(data)\n",
    "\n",
    "# Generate analysis prompt\n",
    "entity_mismatch_analysis_prompt = generate_analysis_prompt(entity_comments)\n",
    "print(entity_mismatch_analysis_prompt)\n",
    "\n",
    "# gemma2:2b-instruct-fp16\n",
    "# llama3.2:3b-instruct-q8_0\n",
    "answer = ollama.generate(\n",
    "    model=\"llama3.2:3b-instruct-q8_0\",\n",
    "    prompt=entity_mismatch_analysis_prompt,\n",
    "    options={'temperature': 0, 'num_predict': 800}\n",
    ")\n",
    "\n",
    "from src.utils.general import wrap_text\n",
    "print(wrap_text(answer['response'], 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAAJ\n",
    "- Move this out into a script after finalising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.evaluate.single_laaj_experiment import LAAJExperiment\n",
    "from src.evaluate.utils import use_llm_to_compare\n",
    "\n",
    "\n",
    "def run_experiment(model_name='qwen2.5:1.5b-instruct-fp16', save_files=False):\n",
    "    experiment = LAAJExperiment(partial(use_llm_to_compare, model=model_name))\n",
    "    results_df = experiment.run_trials()\n",
    "    metrics = experiment.analyse_results(results_df)\n",
    "    fig = experiment.plot_results(metrics)\n",
    "\n",
    "    # Save results\n",
    "    if save_files:\n",
    "        results_df.to_csv('llm_judge_results.csv')\n",
    "        with open('llm_judge_metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        fig.savefig('llm_judge_results.png')\n",
    "    \n",
    "    return results_df, metrics\n",
    "\n",
    "results_df, metrics = run_experiment('gemma2:2b-instruct-fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def use_llm_to_compare(entity1: str, entity2: str, model: str = 'gemma2:2b', provider: str = 'ollama') -> bool:\n",
    "    \"\"\"Compare two medical entities using specified LLM.\"\"\"\n",
    "    query = f\"\"\"Are the phrases \"{entity1}\" and \"{entity2}\" the exact same, synonyms, or similar phrases? \n",
    "    Allow some deviation in phrasing if necessary. Only answer True or False.\"\"\"\n",
    "    \n",
    "    if provider == 'ollama':\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=query,\n",
    "            options={'temperature': 0, 'num_predict': 2}\n",
    "        )\n",
    "        print(response['response'])\n",
    "        return \"True\" in response['response']\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "use_llm_to_compare(\"moderate\", \"moderate\", 'gemma2:2b-instruct-fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.evaluate.multi_laaj_experiment import MultiLAAJExperiment\n",
    "from src.evaluate.utils import use_llm_to_compare\n",
    "\n",
    "def run_experiment(llm_judges, save_files=False):\n",
    "    experiment = MultiLAAJExperiment(llm_judges)\n",
    "    results_df = experiment.run_trials()\n",
    "    metrics = experiment.analyse_results(results_df)\n",
    "    fig = experiment.plot_results(metrics)\n",
    "    \n",
    "    # Save results\n",
    "    if save_files:\n",
    "        results_df.to_csv('multi_llm_results.csv')\n",
    "        fig.savefig('multi_llm_results.png')\n",
    "    \n",
    "    return results_df, metrics\n",
    "\n",
    "# Define model configurations\n",
    "llm_judges = {\n",
    "    'smol0.36b': partial(use_llm_to_compare, model='smollm:360m-instruct-v0.2-fp16'), \n",
    "    'qwen0.5b': partial(use_llm_to_compare, model='qwen2.5:0.5b-instruct-fp16'),\n",
    "    'qwen1.5b': partial(use_llm_to_compare, model='qwen2.5:1.5b-instruct-fp16'),\n",
    "    'gemma2b': partial(use_llm_to_compare, model='gemma2:2b-instruct-fp16')\n",
    "}\n",
    "\n",
    "results_df, metrics = run_experiment(llm_judges, save_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maybe i should check llm as a judge\n",
    "# \"Mild chronic allograft nephropathy\" vs \"Mild chronic changes only\"\n",
    "from src.evaluate.laaj import use_llm_to_compare\n",
    "\n",
    "# (real_string, predicted_string)\n",
    "chronic_change_pairs = [\n",
    "    (\"0\", \"mild\"), # false... correct\n",
    "    (\"Marked\", \"mild\") # false... correct\n",
    "]\n",
    "\n",
    "# (real_string, predicted_string)\n",
    "diagnosis_string_pairs = [\n",
    "    (\"Mild chronic allograft nephropathy\", \"Mild chronic changes only\"), # true\n",
    "    ( \"MARKED CHRONIC CHANGES WITH SEVERE CHRONIC VASCULAR CHANGE\", \"Marked chronic changes with severe chronic vasc   ular change\"), # true\n",
    "    (\"No rejection, mild chronic allograft nephropathy\", \"Mild chronic allograft nephropathy\"), # this is true... should it be?\n",
    "    (\"Borderline rejection\", \"Borderline rejection changes\"), # this is false... wrong\n",
    "    ( \"Borderline rejection, mild chronic allograft nephropathy\", \"Borderline acute rejection and mild chronic allograft nephropathy\"), # false... wrong\n",
    "    (\"CHRONIC ALLOGRAFT NEPHROPATHY\", \"Severe chronic allograft nephropathy\"), # true... should it be?\n",
    "    (\"Acute rejection 1A\", \"Mild chronic changes with superimposed acute cellular rejection, grade 1A\"), # true... correct\n",
    "    (\"No rejection, mild chronic allograft nephropathy\", \"mild chronic allograft nephropathy and mild cyclosporin effect\"), # true... should it be?\n",
    "    (\"Very mild borderline rejection\", \"Very mild borderline acute rejection changes only\") # true\n",
    "]\n",
    "# a lot of this is making me think we need separate strings for rejection type and final diagnosis with them left blank if issues\n",
    "\n",
    "for real_string, pred_string in chronic_change_pairs:\n",
    "    print(f\"{use_llm_to_compare(real_string, pred_string, llama_cpp=False)} for \\t \\\"{real_string}\\\" similar to \\\"{pred_string}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting summary statistics for the corpus\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "def plot_corpus_statistics(data: List[Dict]):\n",
    "    df = pd.DataFrame(data)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Numeric distributions\n",
    "    numeric_cols = ['n_total', 'n_segmental', 'n_global']\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        sns.histplot(data=df, x=col, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Distribution of Glomeruli Counts')\n",
    "    axes[0, 0].legend(numeric_cols)\n",
    "    \n",
    "    # Boolean distributions\n",
    "    bool_cols = ['cortex_present', 'medulla_present', 'abnormal_glomeruli', 'transplant']\n",
    "    bool_counts = df[bool_cols].apply(lambda x: x.value_counts().to_dict())\n",
    "    bool_df = pd.DataFrame(bool_counts).T\n",
    "    bool_df.plot(kind='bar', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Distribution of Binary Features')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Chronic change distribution\n",
    "    sns.countplot(data=df, x='chronic_change', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Distribution of Chronic Change')\n",
    "    axes[1, 0].tick_labels = plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Diagnosis distribution\n",
    "    sns.countplot(data=df, y='diagnosis', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Distribution of Diagnoses')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def generate_summary_stats(data: List[Dict]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    numeric_stats = df[['n_total', 'n_segmental', 'n_global']].describe()\n",
    "    categorical_counts = {\n",
    "        col: df[col].value_counts().to_dict() \n",
    "        for col in ['chronic_change', 'diagnosis']\n",
    "    }\n",
    "    boolean_counts = {\n",
    "        col: df[col].value_counts().to_dict()\n",
    "        for col in ['cortex_present', 'medulla_present', 'abnormal_glomeruli', 'transplant']\n",
    "    }\n",
    "    \n",
    "    return numeric_stats, categorical_counts, boolean_counts\n",
    "\n",
    "# Usage\n",
    "def analyse_corpus(data: List[Dict]):\n",
    "    fig = plot_corpus_statistics(data)\n",
    "    numeric_stats, cat_counts, bool_counts = generate_summary_stats(data)\n",
    "    return fig, numeric_stats, cat_counts, bool_counts\n",
    "\n",
    "fig, nums, cats, bools = analyse_corpus(predicted_json[0:10])\n",
    "fig.savefig('corpus_stats.png')\n",
    "print(\"Numeric statistics:\\n\", nums)\n",
    "print(\"\\nCategory distributions:\\n\", cats)\n",
    "print(\"\\nBoolean distributions:\\n\", bools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import ollama\n",
    "\n",
    "class KidneyBiopsyPredictor:\n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.rf_model = RandomForestClassifier(random_state=42)\n",
    "        self.feature_cols = [\n",
    "            'cortex_present', 'medulla_present', 'n_total',\n",
    "            'n_segmental', 'n_global', 'abnormal_glomeruli',\n",
    "            'chronic_change_encoded'\n",
    "        ]\n",
    "    \n",
    "    def prepare_features(self, data: List[Dict]) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Convert boolean strings to actual booleans\n",
    "        bool_cols = ['cortex_present', 'medulla_present', 'abnormal_glomeruli']\n",
    "        for col in bool_cols:\n",
    "            df[col] = df[col].map({'True': True, 'False': False})\n",
    "        \n",
    "        # Convert numeric strings to numbers\n",
    "        num_cols = ['n_total', 'n_segmental', 'n_global']\n",
    "        for col in num_cols:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "        \n",
    "        # Handle chronic_change as categorical\n",
    "        self.label_encoder.fit(df['chronic_change'])\n",
    "        df['chronic_change_encoded'] = self.label_encoder.transform(df['chronic_change'])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def train_transplant_predictor(self, data: List[Dict]):\n",
    "        df = self.prepare_features(data)\n",
    "        X = df[self.feature_cols]\n",
    "        y = df['transplant'].map({'True': True, 'False': False})\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        self.rf_model.fit(X_train, y_train)\n",
    "        y_pred = self.rf_model.predict(X_test)\n",
    "        \n",
    "        return classification_report(y_test, y_pred)\n",
    "class LLMDiagnosisPredictor:\n",
    "    def __init__(self, model: str = 'gemma2:2b'):\n",
    "        self.model = model\n",
    "        self.prompt_template = \"\"\"Given a kidney biopsy report with the following findings:\n",
    "        - Cortex present: {cortex}\n",
    "        - Medulla present: {medulla}\n",
    "        - Total glomeruli: {total}\n",
    "        - Segmental sclerosis: {segmental}\n",
    "        - Global sclerosis: {n_global}\n",
    "        - Abnormal glomeruli: {abnormal}\n",
    "        - Chronic change score: {chronic}\n",
    "        - Transplant status: {transplant}\n",
    "\n",
    "        What is the most likely diagnosis? Provide only the diagnosis, no explanation.\"\"\"\n",
    "    \n",
    "    def predict(self, case: Dict) -> str:\n",
    "        prompt = self.prompt_template.format(\n",
    "            cortex=case['cortex_present'],\n",
    "            medulla=case['medulla_present'],\n",
    "            total=case['n_total'],\n",
    "            segmental=case['n_segmental'],\n",
    "            n_global=case['n_global'],\n",
    "            abnormal=case['abnormal_glomeruli'],\n",
    "            chronic=case['chronic_change'],\n",
    "            transplant=case['transplant']\n",
    "        )\n",
    "        \n",
    "        response = ollama.generate(\n",
    "            model=self.model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0}\n",
    "        )\n",
    "        return response['response'].strip()\n",
    "    \n",
    "    def evaluate(self, test_cases: List[Dict]) -> Dict:\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        for case in test_cases:\n",
    "            pred = self.predict(case)\n",
    "            predictions.append(pred)\n",
    "            actuals.append(case['diagnosis'])\n",
    "        \n",
    "        return {\n",
    "            'accuracy': np.mean([p == a for p, a in zip(predictions, actuals)]),\n",
    "            'predictions': predictions,\n",
    "            'actuals': actuals\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "def run_prediction_experiments(data: List[Dict]):\n",
    "    # Traditional classifier for transplant prediction\n",
    "    transplant_predictor = KidneyBiopsyPredictor()\n",
    "    transplant_results = transplant_predictor.train_transplant_predictor(data[:10])\n",
    "    \n",
    "    # LLM classifier for diagnosis prediction\n",
    "    llm_predictor = LLMDiagnosisPredictor()\n",
    "    diagnosis_results = llm_predictor.evaluate(data[:10])  # Test on subset\n",
    "    \n",
    "    return transplant_results, diagnosis_results\n",
    "\n",
    "from src.utils.json import load_json\n",
    "results_dir = \"src/renal_biopsy/data/runs/20241214_010916\" \n",
    "predicted_json = load_json(f\"{results_dir}/predicted.json\")\n",
    "transplant_results, diagnosis_results = run_prediction_experiments(predicted_json)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
